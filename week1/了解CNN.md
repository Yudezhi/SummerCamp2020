<center><h1>了解CNN</h1></center>

> CNN 在图像描述领域多用于对图像特征的抽取，关于CNN的经典教程数不胜数，本文决定不再狗尾续貂，只是对相关背景、概念做一个简单的介绍，重点在于推荐几个经典的教程以飨读者。



## 认识图像

图像一般分为彩色图像和黑白图像。注意，这里的黑白图像并非是非黑即白，也是有灰色的。

1. 黑白图像

   黑白图片可以由一个矩阵来存储，灰度值是（0，255），如果像素点非黑即白，那么整个矩阵除以255，则有一个0，1矩阵来表示。

2. 彩色图像

   彩色图片由三个矩阵来存储，分别为RGB，也是图片的三个通道。RGB三个数值来共同描述一个像素点的颜色。



拓展： 在前端开发过程中，经常会遇到颜色的设置，颜色设置通常有两类，一类是十六机制表示，另一种是RGB表示，举例如下：

* 黑色：#000000  ==   rgb(0,0,0)   其中#00 就表示0
* 粉红：#FFC0CB  ==  rgb(255,192,203)  其中C0 表示 192



## 相关概念

#### 卷积

卷积就是用一个小矩阵不断的在大矩阵（这里就是图像的表示）上移动做卷积（对应相乘求和），以得到一个新的矩阵，新的矩阵的维度和数值都是崭新的。下面我们看一个图就明白了！（图源见经典教程3）

![卷积示意图](http://resource.mahc.host/img/Convolution_schematic.gif)

#### 填充（Padding）

我们通常因为一些目的需要让输出的矩阵的维度和输入矩阵相同，这个时候就在新矩阵的周围附加需要个数的0以达到目的。这个过程称为填充。

#### 步长

上面我们讲小窗口在大矩阵上移动做卷积，小矩阵（窗口）每次移动的距离，默认是一步，也可以是自定义的，这个每次移动的“格子”的个数称为步长。由于步长的自定义，当移动过程中格子数不能整除时，一般规定卷积向上取整，池化向下取证。

#### 池化（poling）

关于池化的认识我们从两个角度来看。

1. 池化的作用

   滤波器（小矩阵、小窗口）在窗口滑动过程中重叠计算了很多冗余信息，通过池化来去除，加快计算，防过拟合。

2. 池化的操作

   从具体操作层面来讲，常见的**最大池化**是用另一个小窗口（透明窗口，本身无值）去不断的移动扫描矩阵，依次找出窗口中最大的数字来组成一个新的矩阵。这个过程即为  最大池化操作。

#### 深度

这里的深度是指输出层图片的深度，滤波器的个数决定输出层图像的深度。在这里需要强调的是，输入图像的通道数和滤波器的通道数是一样的，最后特征图的通道数也取决于滤波器的个数



## 整体描述

#### 网络结构

整体网络结构可以看作三部分，分别是卷积层、池采样层、全连接层

#### 三个关键操作

卷积神经网络主要有三个关键的操作，分别是 局部感受野（小窗口）、权值共享、池化

#### CNN的变种

1. AlexNet： 5个卷积层，最大池化，三个全连接层，赢得2012年ILSVRC竞赛
2. GoogleNet ： 赢得2014年ILSVRC竞赛
3. ResNet：称为残差网络，主要创新点在于短接以达到长期记忆，赢得2015年ILSVRC竞赛
4. VGG： Oxford的Visual Geometry Group的组提出的，是2014年Images net上的相关工作，VGG有两种结构，分别是VGG16和VGG19，二者 本质上没有区别，只是深度上不一样



## 经典教程

1. [深度学习、图像分类入门，从VGG16卷积神经网络开始](https://blog.csdn.net/Errors_In_Life/article/details/65950699)
2. [CNN的可视化](https://poloclub.github.io/cnn-explainer/)
3. [理解自然语言中的卷积神经网络](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/#more-348)
4. [AI学习笔记——卷积神经网络（CNN）](https://www.jianshu.com/p/49b70f6480d1)
5. [卷积神经网络简介（PyTorch版）](https://github.com/zergtant/pytorch-handbook/blob/master/chapter2/2.4-cnn.ipynb)

​	

## 参考资料

见上述经典教程