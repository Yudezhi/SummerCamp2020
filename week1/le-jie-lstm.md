# 了解LSTM

> 致敬 ： Hochreiter, S. and J. Schmidhuber, Long short-term memory. Neural computation, 1997. 9\(8\): p. 1735-1780
>
> 作者 Schmidhuber 的名字不会读不要紧，只需要知道人很帅，被称为 LSTM之父

## 简介

LSTM（Long Short Term Memory Networks）中文译作长短期记忆网络，该结构目前非常流行，主要用来接收特征而生成序列化结构，在图像描述生成领域，LSTM在输入端接收图像特征等向量，然后根据相关状态不断的生成（输出）单词，最后组成一句话。LSTM在类别划分上属于RNN的变种，因此我们先简要介绍RNN，然后再谈LSTM。

## RNN

> RNN 全称 Recurrent Neural Network，中译为 循环神经网络。

### RNN是干什么的

RNN多用于自然语言相关的任务，它本身就有自己的结构，只不过因为一些缺点，渐渐的被LSTM等它的变种所替代。RNN的主要功能是建模和生成文本\(出处见教程1\)，通过语言模型，我们就可以通过给定单词，让模型来输出以假乱真的文本，这个过程类似于我们通过别人的上半句，去猜他后面会说什么一样，本质上都是一样的，因为我们听的够多的这个过程中，就建立了一个很好的语言模型。

亦或者，我们可以把RNN看作是一个复杂的函数，就是一个神经网络而已，在不断的训练下，这个函数拥有了一些参数，这些参数就可以把一个输入转换成另一个形式的输出。例如机器翻译就是把语言A转换成语言B，语音识别是把声音转换成文字，图像描述是把图像转换成文字，后两个例子的特殊之处只不过输出和输出不是同一个模态，所以还需要做一些模态转换上的工作。

### 类比解释

循环神经网络的设计和人类的经验还是有大的相似之处的。我们人类和机器有时候是很像的，我们接收信息，在大脑中做出处理，然后做出动作来反馈。我们可以把RNN的动作过程看作是一个成语接龙，刚开始接收裁判给出的首字类似于RNN开始接收到的初始特征向量，然后根据首字给出自己的答案，后续玩家根据上一位玩家的答案来给出自己的答案，只不过这里RNN是自己跟自己玩。

在这里，我们需要引出记忆这个概念。RNN 可以根据上一位玩家的答案来给出自己的答案，说明他记住了上一位玩家的结果，这就是一种记忆，但记住上一位玩家的答案可不够，因为成语接龙要求后面的答案和前面的答案不能重复，所以需要记住自己之前所有玩家的结果，然后再根据这些去给出自己的答案。解释到这里，RNN的基本结构已经呼之欲出了。

### RNN的结构

下面我们直接来看经典的RNN图示（图源见教程1）：

![RNN&#x7B80;&#x56FE;](http://resource.mahc.host/img/10.png)

图中的 $$X_t$$ 即表示在 t 时刻的一个输入，$$h_t$$ 表示输出，图中的黑色回路线表示这个图是有循环功能的，我们把上图展开，其实就是变成了一个有时间顺序的序列操作，见下图：

![RNN&#x5C55;&#x5F00;&#x56FE;](http://resource.mahc.host/img/11.png)

现在我们来看，$$X_0$$ 就是成语接龙的开始，$$X_1$$可以看作是前一个人的输出，而$$h_0$$ 则可以看作是代表着一些历史信息，这种历史信息在$$h_2 , h_3 ....$$ 等更加明显，一般称之为隐藏状态（hidden state），类似于哪些成语已经被说过了。绿色方块A之间的连接黑色箭头，就是 $$h_*$$ , 以上这种解释类比只是一家之言，只为了读者可以更快的理解这种设计意图，更专业的解释见相关论文。

在这里我想初学者还有三个疑问， $$X_t$$ 究竟是什么？这种结构如何停止？ 绿色方块A中发生了什么？

关于第一个问题，在数学表示上，$$X_t$$ 就是一个向量，当 $$t=0$$时，其可以是图像的特征，而 当$$t\geq 1$$时，$$X_t$$ 可以表示上一状态的输出特征，当然这种解释是最简单的，后续有的模型或许会有更多样的设计，但我们可以基本确定的是他代表着特征。关于如何理解特征，笔者认为可以把其看作是对世间万事万物的编码，只不过这种编码的作用不仅仅是区分，还有保证同类事物在编码上的相似性。

至于第二个问题，初学者只需要知道在不断的生成过程中，因为训练的缘故，最后会生成一个结束符，这时候整个“生产”过程就自动停止了。

第三个问题，就是RNN的核心了，问题等价于$$h_t$$ 如何计算？我们给出RNN的数学表达（来源于官网）：

$$h_t = tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)}+b_{hh})$$

这里的$$x_t$$ 就是我们的输入，$$h_{(t-1)}$$ 表示上一状态的隐藏状态，$$W_{**}，b_{**}$$$分别是权重和偏置，也是训练过程中需要不断优化的值。

## LSTM

上面我们简单介绍了RNN，下面的LSTM的学习也会变的简单很多。

上面我们看到的$$h_t$$ 就是所谓的记忆，理论上讲，后面的单元可以通过这个值记住前面所有的信息，但是在实际应用中却效果不好，无法记住比较“远”的信息，因此有了改进版的。同理，我们直接上经典模型图：

![LSTM](http://resource.mahc.host/img/LSTM3-chain.png)

通过上图，我们先不要看里面的具体符号，而是整体上来看，是不是和RNN在序列上十分相似，只不过单元之间的传递多了一个箭头,也就是说，LSTM和RNN相比，区别在于内部计算方式变了而已，而整体的工作方式还是一样的。关于内部具体是如何计算的，请务必参考经典教程中的教程2，实乃大家之作，我在此处不再赘述。

这里，我再给出一个经典动图（源于教程1）和计算公式，以便读者复习之用。

LSTM 计算公式如下：

> $$f_t= \sigma(W_f·[h_{t-1},x_t] + b_f)$$
>
> $$i_t = \sigma (W_i·[h_{t-1},x_t]+b_i)$$
>
> $$\tilde{C}_t = tanh(W_C·[h_{t-1},x_t]+b_C)$$
>
> $$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$$
>
> $$o_t = \sigma(W_o[h_{t-1},x_t]+b_o)$$
>
> $$h_t = o_t * tanh(C_t)$$

运算过程动图展示：

![LSTM&#x52A8;&#x56FE;](http://resource.mahc.host/img/lstm.gif)

## 经典教程

1. [循环神经网络](https://github.com/zergtant/pytorch-handbook/blob/master/chapter2/2.5-rnn.ipynb)
2. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
3. [教程2翻译版](https://www.jianshu.com/p/4b4701beba92)

## 参考资料

见经典教程部分。

