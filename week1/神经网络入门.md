<center><h1>神经网络入门</h1></center>

> 本节内容建议直接看周志华的西瓜书《机器学习》第五章内容效果更好，因才疏学浅，本教程仅针对这一部分的概念做简要概括。请务必学好这一节，对后期理解论文等很有帮助。
>
> 神经网络（Neural Network）的研究已经有很长时间了，尤其在深度学习大行其道的今天，神经网络更是一个相当广泛的概念，不同的学科可能有不同的定义。在计算机深度学习的背景下，大家普遍采用1988年 Kohonen 的描述“神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应”。



## 概述

​		神经网络还是启发于生物神经网络，所以有很多相似之处。通过高中知识我们知道，神经元之间传递信号靠的是电位的变换。有一个小的知识点就是不是有电就直接传递化学物质，而且电位还得高于某个阈值，神经元高于某个电位后开始传递信号时我们称之为 **激活【activation】** 。激活正是神经网络中不可或缺的重要一环。

​		更确切的讲，在计算机中，我们定义一些节点，让这些节点相互连接，然后让他们之间按照某个约定的规律传递数据，这些约定的规律中就蕴含着一些参数，你可以把这些规则简单理解为一个复杂的函数，而需要输出的数值就要通过一个阈值比较，最后通过“激活函数”处理并输出。

## 感知机

​		感知机（Perceptron）由两层神经元构成，分别称为输入层和输出层，图示如下：

![两个输入神经元的感知机网络结构示意图（图源：周志华《机器学习》图5.3）](http://resource.mahc.host/img/image-20200703220840371.png)





## 神经网络

​		多层神经元相连构成神经网络，输入层和输出层之间的所有神经元称为隐藏层，输入、输出层都只有一层，而隐藏层可以有很多层（注：输出层在某些网络如GoogleNet 中有多层）。层与层之间的神经元可以是密集连接的，而连接的强弱即为权重，每个神经元的阈值和这些权重的值，就是我们训练的目标！下图中的蓝色圆点就是隐藏层，这里有5个隐藏层。



![神经网络示意图](http://resource.mahc.host/img/7.png)

## 激活函数

> 激活函数也叫激励函数，响应函数，或起源于对青蛙神经元的研究。激活函数如果不可导，那么就不能使用梯度进行优化。

#### 介绍

​		通过上面的简单介绍，我们知道激活函数其实就是用来判断信息是否传递给下一层的条件控制。激活函数其实不仅仅是一种启发式设计，而且其还能改变神经网络的线性和非线性。

​		我们可以把神经网络看作是一个庞杂的函数，这个函数可以去描述很多事情的因果关系，而究其本质，如果神经网络没有激活函数，他不过是一些简单的相加和数乘运算，而这只是线性运算，那么我们的神经网络只能去描述线性关系。可是非线性关系或许才是主流，所以引入了非线性激活函数，以此来让我们的神经网络去描述非线性关系，这时我们可以把神经网络称为非线性函数，甚至可以证明神经网络在一定的深度下，满足一定的神经元个数就可以适配任意函数。

#### 常见激活函数

1. ReLU(Rectified Linear Unit)  

   >  Relu=max(0,x)
   >
   > 又称线性修正单元，通常指代以斜坡函数（负半轴为0，正半轴像斜坡）及其变种为代表的非线性函数。优点是在大于0时可以保持梯度不变，是神经网络内常用的激活函数。而缺点在于有时候因为学习率等设置，训练过程中很多神经元会“死掉”，即不再参与更新。

2. sigmoid

   >$$f(x)= \frac{1}{1+e^{-x}} $$
   >
   >优点在于适合讲输入分为两类，梯度好控制，可导，形状缓和，而缺点在于可能在极端情况下梯度为0，会有梯度弥散的情况。

3. tanh(hyperbolic tangent)

   > $$ tanh(x)=\frac{e^{x}+e^{-x}}{e^{x}+e^{-x}}$$
   >
   > 双曲正切函数相较于sigmoid，tanh函数的输出更陡，在原点附近和 y=x 形式相近，某些情况下训练相对容易，常在RNN中使用。



下图为常见激活函数的二维平面图，来源见水印：

![常见激活函数示意图](http://resource.mahc.host/img/clip_image001.jpg)





## 参考资料

1. 周志华 《机器学习》
2. PyTorch之[神经网络简介](https://github.com/zergtant/pytorch-handbook/blob/master/chapter2/2.3-deep-learning-neural-network-introduction.ipynb)
3. ReLU [维基百科](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))   [百度百科](https://baike.baidu.com/item/ReLU%20%E5%87%BD%E6%95%B0/22689567?fr=aladdin)
4. 常见[激活函数介绍](https://www.jianshu.com/p/857d5859d2cc)

