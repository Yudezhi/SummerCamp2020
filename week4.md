# 第四周：答辩报告

## PPT版

![image-20200731193548507](http://resource.mahc.host/img/image-20200731193548507.png)

![image-20200731194015272](http://resource.mahc.host/img/image-20200731194015272.png)

![image-20200731194149289](http://resource.mahc.host/img/image-20200731194149289.png)

![image-20200731194206068](http://resource.mahc.host/img/image-20200731194206068.png)

![image-20200731194022347](http://resource.mahc.host/img/image-20200731194034575.png)

![image-20200731194218383](http://resource.mahc.host/img/image-20200731194218383.png)

![image-20200731194232200](http://resource.mahc.host/img/image-20200731194232200.png)

![image-20200731194241980](http://resource.mahc.host/img/image-20200731194241980.png)

![image-20200731194255147](http://resource.mahc.host/img/image-20200731194255147.png)

![image-20200731194054624](http://resource.mahc.host/img/image-20200731194113198.png)

![image-20200731194311364](http://resource.mahc.host/img/image-20200731194311364.png)

![image-20200731194321144](http://resource.mahc.host/img/image-20200731194321144.png)

![image-20200731194331288](http://resource.mahc.host/img/image-20200731194331288.png)

![image-20200731194341104](http://resource.mahc.host/img/image-20200731194341104.png)

![image-20200731194454852](http://resource.mahc.host/img/image-20200731194454852.png)

![image-20200731194505942](http://resource.mahc.host/img/image-20200731194505942.png)

![image-20200731194516579](http://resource.mahc.host/img/image-20200731194516579.png)

![image-20200731194526163](http://resource.mahc.host/img/image-20200731194526163.png)

![image-20200731194539980](http://resource.mahc.host/img/image-20200731194539980.png)

![image-20200731194558138](http://resource.mahc.host/img/image-20200731194558138.png)

![image-20200731194609389](http://resource.mahc.host/img/image-20200731194609389.png)

![image-20200731194621150](http://resource.mahc.host/img/image-20200731194621150.png)

![image-20200731194631729](http://resource.mahc.host/img/image-20200731194631729.png)

![image-20200731194641862](http://resource.mahc.host/img/image-20200731194641862.png)

![image-20200731194651530](http://resource.mahc.host/img/image-20200731194651530.png)

![image-20200731194701151](http://resource.mahc.host/img/image-20200731194701151.png)

## 文字版讲稿

P1：各位老师，各位同学，大家下午好，我是D组的潘勉之，今天将由我和李静雯同学带来报告，报告的主题是图像描述简介及其评价指标浅析。

P2：首先我对图像描述领域做一个简单的介绍。

P3：图像描述是NLP……有逻辑，贴合内容。Image Caption正式被提出是在2010年，主要思路就是输入一张图片，模型输出描述该图像的一句话。实际应用方面，Image Caption可用于图像检索，图像归类，帮助盲人理解图像等等。

P4：图像描述主要有如下三种方法，其中第三种基于深度学习的方法是数据驱动的方法。需要大量标注数据，目前常用的数据库有COCO和Flicker两类。最近这一阶段该领域仍有许多进展，比如今年发表的这篇论文，将语言的预训练模型引入到图像描述和视觉问答中，并且这两个任务使用的是同一个模型，取得了很好的效果。

P5：第二部分将介绍图像描述的一些主流方法。

P6：2015年的这篇show and tell论文，借鉴了当时机器翻译领域效果出色的”encoder-decoder”模型，利用CNN对图片编码，再由RNN解码得到最终的描述语句，构成了一个端对端的模型。

模型具体使用的CNN是GoogLeNet，解码器部分则使用的是LSTM，具体到每个time step，文章还使用了BeamSearch算法，有利于得到更好的结果。

P7：同年的这篇Show, Attend and Tell在之前Show and tell模型的基础上引入了注意力的机制。与之前不同的是图像不同位置会被赋予一定的权重，并在每个time step输入到解码器中，实现了对图片对象更精准的描述。

文中实现了soft attention和hard attention两种机制，这张图也揭示了两者的区别，hard机制在注意的位置权重为1，其余部分均为0；而soft机制注意的位置相对较广，只不过不同位置注意程度不同。

P8：此外还有adaptive, BUTD等模型；为衡量模型生成语句的好坏，人们设计了各种指标，包括CIDEr, Spice等等。

P9：第三部分主要是我们做的人工标注实验及对应的数据分析。

P10：在阅读show and tell这篇论文时，我们发现在一些指标下，比如B Four和CIDER，模型生成的描述语句得分甚至要高于人工标注，出于怀疑，我们也计算量COCO数据集的人工标注的得分，发现在B Four指标下得分较低，于是我们产生了这样的问题：模型生成的图像描述真的由于人工标注吗？机器评价指标是否能完全反应图像描述的质量？

P11：由于人工评价准确性更高，我们预计使用人工评价作为句子质量的真实反映。我们将从对象完整性，关系完整性，属性完整性三个角度对模型生成的句子评分，三者占最终分数的比例分别为50%,30%,20%

P12：最终我们回收了701个图片样本，每张图片被标注了三次，得到了NIC，NIC\_pretrain等五个模型的人工评价分数，可以看到BUTD分数最高，NIC\_pretrain分数最低。

P13：对各个模型比较发现，在人工评价下，Adapative模型质量是要低于Soft\_att的，但无论在哪个机器指标下，结果却相反，这就初步说明：机器评价分数的提高并不总是意味着图像描述质量的提高。

P14：最后，我们还比较了不同模型在各个角度下的质量，得到了这样的结果，可以看到BUTD模型无论在哪一方面效果都是最好的。

我介绍到的部分大概就是这样，下面请另一位同学介绍一下在人工标注的过程中具体发现的问题。

大家好，我是计算机科学与技术系本科生李静雯，下面由我来继续分享在我们人工标注过程中遇到的一些问题，和相关的思考。

（P15）

首先呢，是语言丰富性的问题。

（P16）

对于这张图片，参考的人工标注给出了很贴切的描述，但是我们来看右边的机器标注，全都是sitting on top of the table，这就使得我们的语言缺少了丰富性和准确性，生成出来的自然语言就不那么”自然“了。

（P17）

除了这种动词表达上的僵硬，对场景的描述也会陷入一种“套路性的描述“。在我们标注过的图片中，但凡遇到原野，绝大部分的句子都是a lush green field，然后对于雪的描述又往往是“a snow covered slope”，在这种描述模式下，你看上十来张图片就会觉得千篇一律，特别没有新意。

（P18）

第二个问题就是，我们现有的模型，几乎没有一个去处理图片上文字信息的能力。

比如说这张图片，我们作为一个人类，看到这么奇怪的建筑，是一定会去注意那个楼上的，算是招牌的信息的，然后我们才能知道这个房子是干什么用的，但是我们的模型没有这样去做，他们给出的描述就是很简单，完全没有说明这个房子的用途，其实可以说，它丢失了对图像描述非常非常重要的信息。

（P19）

但是我们可以注意到，stop路牌被识别了，并且在其它图片中，也是可以被识别的。但是我并不认为是stop这个词被抓取到了，而是说其它特征被成功地识别出来，比如说颜色和形状，毕竟所有stop路牌都是这个样子。

（P20）

第三个问题是上下文关联导致的错误。

首先是动词的习惯性选择。在很多图片中，“人+雪橇”这样的组合都是和“滑雪”这个动作来匹配的，但是在这张图上，人没有在滑雪，模型还是习惯性地生成了“ski”，所以说LSTM中上下文对候选词的影响也不是完全积极的。

（P21）

接下来，是描述对象无中生有的情况。在很多张卫生间内景的图片中，都莫名其妙地被添加了一个mirror，它的原因可能是LSTM根据“卫生间”这一特征给出了对“mirror” 较好的预估，把一个和mirror相似的对象误认成了mirror

（P22）

在一些比较特别的图片里面，主题对象之间并没有什么关系，比如说这张图，把candle插在一个parking meter上，结果就是所有模型都没有生成出来candle这个词。我觉得这个也是上下文的问题，也就是candle和parking meter的关系太不紧密，结果candle就直接被除去了。

（P23）

最后一个问题是描述对象的重复。

我们发现在很多图片的描述中，同一个名词会在句子里面出现不止一次，结果使得句子完全没法被理解，就像这里的light house，想要解决这个问题，也许可以加入适当的惩罚因子，但是会不会带来负面效应还是要看实验。

（P24）

略

