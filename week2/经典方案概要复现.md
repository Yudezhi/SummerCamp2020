# 经典方案概要复现

> 本部分针对图像描述基于深度学习的方法中最为朴素、经典的论文《 Show, attend and tell: Neural image caption generation with visual attention》进行复现。
>
> 该论文是基于论文《show and tell: a neural image caption generator》而增加了注意力机制，大大的提高了相关评测指标，是此领域一个里程碑工作。
>
> **本文件仅仅针对在复现过程中需要用到的重要功能进行了代码展示，目的在于使读者了解代码功能与复现框架，熟悉代码风格，然后尽力搭建自己的复现模型。读者直接根据这些代码整合出可以运行的项目会有较大难度（此处并未给出所有代码），后续在读者自行尝试后会给出完整代码供再次阅读和运行**



## 论文模型

​		本文最重要的贡献就是应用了两种注意力机制在图像描述上，分别称为软注意力（soft attention）和硬注意力（hard attention）。

​		软硬注意力的区别在于，对于某特征向量，hard attention要么对他的权重只有0或者1 这两个选项。而soft attention则是从0到1 的变量。本次仅以soft 注意力为代表进行实现，因为这种实现应用更加的广泛。

![图1： 模型概览图（图源：见文末参考资料1）](http://resource.mahc.host/img/figure2.png)



​		本文模型主要由三部分组成，第一部分是图像特征的抽取，本部分主要采用VGG来提取。模型第二部分对图像特征进行关注。对抽取出的图像特征的每个权重的计算过程，见图3：

![图2：注意力模块权重参数的计算（图源：见文末参考资料1）](http://resource.mahc.host/img/figure3.png)



在注意力参数计算出来以后，在对图像特征求加权和即可得图像得上下文表示向量，记作z.

​		模型第三部分将得到得上下文向量z送入LSTM中进行结果预测，最终得到结果。

![图3：注意力模型示意图（图源：见文末参考资料1）](http://resource.mahc.host/img/figure4.png)



## 模型复现

### 实验数据

​		本文所用的实验数据为MS COCO2014 ，与论文中一致，不同的是采用Karpathy (Karpathy et al. Deep visual-semantic alignments for generating image descriptions. )分割方法，即验证集和数据集各采用5000张图像，其余数据全部作为训练集。

### 核心代码

​		本部分仅展示主要功能代码，详细请申请查看原文件。相关代码大部分来源于开源工作的二次改进。

#### 数据预处理

* 数据集Karpathy分割（常用）

  > 希望读者通过这个小功能文件，学会对数据集进行操作

  ```python
  # # Karpathy Split for MS-COCO Dataset
  # 合并原始train val集以获得更多的训练数据，保留val中的5000个作为新val，保留test中的5000个作为新test
  # 生成三个集合的json文件
  import json
  from random import shuffle, seed
  
  seed(21)  # Make it reproducible ,not vital to read here
  
  num_val = 5000
  num_test = 5000
  
  val = json.load(open('../data/annotations/captions_train2014.json', 'r'))
  train = json.load(open('../data/annotations/captions_val2014.json', 'r'))
  
  # Merge together
  imgs = val['images'] + train['images']
  annots = val['annotations'] + train['annotations']
  
  shuffle(imgs)
  
  # Split into val, test, train
  dataset = {}
  dataset['val'] = imgs[:num_val]  # img[k]包括某图片的全部信息，如图像路径和图片id
  dataset['test'] = imgs[num_val: num_val + num_test]
  dataset['train'] = imgs[num_val + num_test:]
  
  # Group by image ids
  itoa = {}  # 以image_id查找annots信息的词典，包括全部数据
  for a in annots:
      imgid = a['image_id']
      if not imgid in itoa: itoa[imgid] = []
      itoa[imgid].append(a)
  
  json_data = {}
  info = train['info']
  licenses = train['licenses']
  
  split = ['val', 'test', 'train']
  
  for subset in split:
  
      # images存储所有图片的img信息，annotations存储annots的信息
      json_data[subset] = {'type': 'caption', 'info': info, 'licenses': licenses,
                           'images': [], 'annotations': []}
  
      for img in dataset[subset]:
          img_id = img['id']
          anns = itoa[img_id]
  
          json_data[subset]['images'].append(img)
          json_data[subset]['annotations'].extend(anns)
  
      # 写为三个json文件
      json.dump(json_data[subset], open('../data/karpathy_split_' + subset + '.json', 'w'))
  
  print('*** Complete ***')
  ```

* 生成词典

  ```python
  import nltk
  import pickle
  import argparse
  
  from collections import Counter
  from pycocotools.coco import COCO
  
  class Vocabulary(object):
      # 单词表
      def __init__(self):
          self.word2idx = {}
          self.idx2word = {}
          self.idx = 0
  
      def add_word(self, word):
          if not word in self.word2idx:
              self.word2idx[word] = self.idx
              self.idx2word[self.idx] = word
              self.idx += 1
      def __call__(self, word):
  
          # use <unk> to replace unknown word (用<unk>代替未登录词)
          if not word in self.word2idx:
              return self.word2idx['<unk>']
          return self.word2idx[word]
  
      def __len__(self):
          return len(self.word2idx)
  
  ```
  
