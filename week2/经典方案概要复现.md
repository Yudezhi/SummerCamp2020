# 经典方案复现

> 本部分针对图像描述基于深度学习的方法中最为朴素、经典的论文《 Show, attend and tell: Neural image caption generation with visual attention》进行复现。
>
> 本论文是基于论文《show and tell: a neural image caption generator》而增加了注意力机制，大大的提高了相关评测指标，是此领域一个里程碑工作。
>
> ==针对初学者，本文内容还很单薄，后期将继续补充==



## 论文模型

​		本文最重要的贡献就是应用了两种注意力机制在图像描述上，分别称为软注意力（soft attention）和硬注意力（hard attention）。

​		软硬注意力的区别在于，对于某特征向量，hard attention要么对他的权重只有0或者1 这两个选项。而soft attention则是从0到1 的变量。本次仅以soft 注意力为代表进行实现，因为这种实现应用更加的广泛。

![图2： 模型概览图（图源：见文末参考资料1）](http://resource.mahc.host/img/figure2.png)



​		本文模型主要由三部分组成，第一部分是图像特征的抽取，本部分主要采用VGG来提取。模型第二部分对图像特征进行关注。对抽取出的图像特征的每个权重的计算过程，见图3：

![图3：注意力模块权重参数的计算（图源：见文末参考资料1）](http://resource.mahc.host/img/figure3.png)



在注意力参数计算出来以后，在对图像特征求加权和即可得图像得上下文表示向量，记作z.

​		模型第三部分将得到得上下文向量z送入LSTM中进行结果预测，最终得到结果。

![图4：注意力模型示意图（图源：见文末参考资料1）](http://resource.mahc.host/img/figure4.png)



## 模型复现

### 实验数据

​		本文所用的实验数据为MS COCO2014 ，与论文中一致，不同的是采用Karpathy (Karpathy et al. Deep visual-semantic alignments for generating image descriptions. )分割方法，即验证集和数据集各采用5000张图像，其余数据全部作为训练集。

### 核心代码

​		本部分仅展示主要功能代码。

**VGG编码器**：

```python
class Encoder(nn.Module):
    ''' encoder of cnn to represent images'''

    def __init__(self, type='vgg19_fn'):
		super().__init__()
        self.cnn, self.avgpool = self.cre_cnn(type)
        
    def forward(self, image):
        batch_size = image.size(0)
        features = self.cnn(image)  # (b,512,14,14)
        fea_vec = self.avgpool(features).view(batch_size, -1)  # (b,512)
        fea_maps = features.permute(0, 2, 3, 1).view(batch_size, 196, -1)#14*14=196
        return fea_vec, fea_maps
	# 下载模型
    def cre_cnn(self, type):
        '''select cnn model'''
        if type == 'vgg19_fn':
            # (b, 512, 14, 14)
            print("Downloading pretrained vgg19_fn from torchvsion.models...")
            cnn = models.vgg19_bn(pretrained=True)
            print("Done.")
            modules = list(list(cnn.children())[:-2][0].children())[:-1]
            for p in cnn.parameters():
                p.requires_grad = False
            return nn.Sequential(*modules), nn.AvgPool2d(14)

```

**Soft Attention**：

```python
class Attention(nn.Module):
    '''Attention module'''
    def __init__(self, conf):
        super().__init__()
        self.fea_att = nn.Linear(conf['model']['embed_dim'], conf['model']['att_dim'])
        self.hid_att = nn.Linear(conf['model']['hid_dim'], conf['model']['att_dim'])
        self.att = nn.Linear(conf['model']['att_dim'], 1)
        print("Init Attention Done.")

    def forward(self, fea_maps, hidden):
        fea_att = self.fea_att(fea_maps)  # (b,196,512) -> (b,196,100)
        hid_att = self.hid_att(hidden)  # (b,512) -> (b,100)
        fusion = fea_att + hid_att.unsqueeze(1)  # (b,49,100)
        att = self.att(torch.relu(fusion)).squeeze(2)  # eq4: eti = fatt(ai,ht-1) (b,196)
        alpha = torch.softmax(att, 1)  # eq5: softmax
        z = (fea_maps * alpha.unsqueeze(2)).sum(1)  # (b,512)
        return z, alpha
```

**LSTM解码器**：

```python
class SoftDecoder(Decoder):#继承本地实现的decoder

    def __init__(self, conf):
		#略

    def forward(self, fea_vec, fea_maps, cap, cap_len):
        batch_size = fea_vec.size(0)
        h, c = self.fea_init_state(fea_vec)
        vocab_size = self.vocab_size
        embeddings = self.embed(cap)#获取caption的编码矩阵
        weights = torch.zeros(batch_size, max(cap_len), vocab_size).to(device)
        alphas = torch.zeros(batch_size, max(cap_len), 196).to(device)
        betas = torch.zeros(batch_size, max(cap_len), 1).to(device)
        for t in range(max(cap_len)):
            z, alpha = self.attention(fea_maps, h) #att (b,512) z加权特征，alpha权重系数
            beta = torch.sigmoid(self.fb(h))
            z = beta * z
            h, c = self.lstmcell(torch.cat([embeddings[:, t, :], z], 1),(h, c))
            weight = self.fc(self.dropout(h))
            weights[:, t, :] = weight
            alphas[:, t, :] = alpha
            betas [:, t ,:] = beta
        return weights, alphas, betas

```



## 复现结果

<center>表1：复现结果对比</center>

| 模型   | Bleu_1 | Bleu_2 | Bleu_3 | Bleu_4 | METEOR | ROUGE_L | CIDEr | SPICE |
| ------ | ------ | ------ | ------ | ------ | ------ | ------- | ----- | ----- |
| Source | 70.7   | 49.2   | 34.4   | 24.3   | 23.90  | —       | —     | —     |
| Ours   | 79.8   | 68.2   | 56.1   | 46.5   | 32.4   | 64.6    | 182.8 | 26.5  |

​		表1 是本次复现的结果对比，`Source` 表示论文中的实验结果，`—`表示无法得知，`Ours` 表示复现结果，可以看到指标有很大的提升，原因主要有两个：一是采用现在通用的Karpathy分割方法后，训练数据大大增加了。二是因为时间跨度等因素，很多 trick 更好用了，并且词汇表采用10117大小而非论文中的10000.

### 思考	

​		本次复现的模型是Encoder-Decoder框架下结合注意力机制的经典之作，后来的文章在此基础上不断的改进模型，亦或者是修改编码器，以获得更好抽取图像特征或者caption的编码，指标也越来越高。

​		然而，指标越来越高并不意味着质量越来越好。例如自从两阶段训练法问世以来，几乎所有的模型都要经过后来的强化训练阶段，指标的增长变得不再明显，有趣的是有的论文每次以上升0.1个指标点而声称达到了SOTA效果，不乏有灌水的嫌疑！

​		并且，越来越多的人提出指标高了，生成的句子质量却变低了。基于这个现象，我针对指标是否仍有参考价值作了一个小实验进行验证并尝试进行了分析，具体见指标评价思考。

## 参考资料

1. 博客：[图像理解之show attend and tell](https://blog.csdn.net/shenxiaolu1984/article/details/51493673)
2. Vinyals, O., et al. Show and tell: A neural image caption generator. CVPR. 2015.
3. Xu, K., et al. Show, attend and tell: Neural image caption generation with visual attention. in International conference on machine learning. 2015.
4. Karpathy, A. and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. CVPR. 2015.
