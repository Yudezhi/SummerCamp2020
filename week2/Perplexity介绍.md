# Perplexity 介绍

> 困惑度（perplexity）在使用中不是十分常见，但也会碰到，更重要的是它和我们常用的交叉熵本质上是一样的，本文作简单介绍。



## 简介

困惑度主要是用来评价语言模型（Language Model）的好坏的。而语言模型其实就是来预测一句话是人话的概率，在应用上就是根据前k个词来预测第 k+1 的词，这个词的概率分布表示为： $$p(x_{k+1}|x_1,x_2,...,x_k)$$

语言模型除了上面的困惑度来评价，还可以通过实际应用去评价其效果，但花费时间较长。而困惑度正好是一个能够体现模型本身特点，行之有效的模型质量评测方法。

“困惑度” 这个概念最初来源于信息论，用来衡量一个概率分布或概率模型预测样本的好坏程度，困惑度较低的概率分布模型能够更好的预测样本，也被用来比较两个概率模型的好坏。

下面给出三种困惑度的计算，来源见参考资料3.

## 计算

### 概率分布的困惑度

首先给出公式：
$$
PP(p):= 2^{H(p)}=2^{-\sum_x p(x)log_2 p(x)}
$$
上式中，H(p) 是该分布的熵，x 遍历所有的事件。上面的式子中的2（两个）可以是其他值，例如 e 。

随机变量X的复杂度由其所有的可能取值x定义。一个比较特殊的例子是k面均匀的骰子，它的困惑度是k。 一个拥有k困惑度的随机变量有着和k面均匀骰子一样多的不确定性，并且可以说该随机变量有着k个困惑度的取值。（在有限样本空间离散随机变量的分布中，均匀分布有着最大的熵）

困惑度有时候也被用来衡量一个预测问题的难易程度，但是这个方法不总是精确的。例如你有两个选择，一个的概率是0.9，那么意味着你猜对的概率是90%，那么困惑度就是 $$PP = 2^{-0.9 log _2 0.9 - 0.1 log_2 0.1} = 1.38$$  .

更进一步的，其实指数$$H(p)$$ 就是熵。熵是使用理论上的最佳可变长度代码对随机变量的结果进行编码所需的预期位数或“平均”位数的度量。它可以等效地视为通过学习随机变量的表现而获得的预期信息。



### 概率模型的困惑度

用一个概率模型q 去估计一个未知的概率分布p，那么这时模型q的困惑度就定义如下：
$$
b^{-\frac {1} {N} \sum ^N _1 log_b q(x_i) }
$$
上式中， $$x_1,x_2,...,x_N$$ 是来自甚至概率分布p 的观测值，b 通常取2. 如果上式的值越小，则说明 q 对 p  的拟合越好，即q 对于看到的观察值不那么惊讶（困惑）。

上面的指数部分我们可以看作是对于事件 $$x_i$$  的表述所需要的平均比特数目。（信息熵 致敬香农）

上面的指数部分也可以看作交叉熵，我们这里一并给出交叉熵的公式： $$H(\tilde{p},q) = - \sum^n _{i=1} \tilde{p}(x_i) log(q(x_i))$$ , 这里的 $$\tilde{p}$$ 是根据测试样例的经验分布。

 

### 单词的困惑度

在自然语言处理中，困惑度是评价语言模型的一种方法。一个语言模型就是在一个句子或者文本上的概率分布。（例如一句话上的每个词都有一个概率，或者说这个词还有其他的概率用其他词来填写）

适用概率模型的困惑度，我们会发现，如果句子$$x_i$$ （指位置 i）的信息熵（香农提出来表示信息的不确定性）是190，即这个位置上的句子需要190 比特去编码，则依据(2) 式，它的困惑度式 $$2^{190}$$ 次方，这是一个很大的数。（译者：相当于投掷一个$$2^{190}$$面筛子的不确定性）

通常，我们会考虑句子有不同的长度，所以我们会计算每个分词上的困惑度。比如，一个测试集上共有1000个单词，并且可以用7.95个bits给每个单词编码，那么我们可以说这个模型上每个词有2^(7.95)=247 困惑度。相当于在每个词语位置上都有投掷一个247面骰子的不确定性。

在Brown corpus (1 million words of American English of varying topics and genres) 上报告的最低的困惑度就是247per word，使用的是一个trigram model（三元语法模型）。在一个特定领域的语料中，常常可以得到更低的困惑度。

要注意的是，这个模型用的是三元语法。直接预测下一个单词是”the”的正确率是7%。但如果直接应用上面的结果，算出来这个预测是正确的概率是1/247=0.4%，这就错了。（译者：不是说算出来就一定是0.4%，而是说这样算本身是错的）因为直接预测下一个词是”the“的话，我们是在使用一元语法，而247是来源于三元语法的。当我们在使用三元语法的时候，会考虑三元语法的统计数据，这样做出来的预测会不一样并且通常有更好的正确率
————————————————
以上部分原文链接：https://blog.csdn.net/qq_27825451/article/details/102457058



### 图像描述中的表述

$$
log_2 PPL(w_{1:L}| I) = - \frac {1}{L}  \sum_{n=1}^{L} log_2 P (w_n | w_{1:n-1},I)
$$

上式中，L 是句子的长度，$$I$$  表示一个图像，困惑度越低越好，log 函数不影响多个结果间的比较。



## 推荐阅读

[求通俗解释NLP里的perplexity是什么？](https://www.zhihu.com/question/58482430)



## 参考资料

1. [通俗解释困惑度 (Perplexity)-评价语言模型的好坏](https://zhuanlan.zhihu.com/p/44107044)
2. [语言模型与困惑度](https://blog.csdn.net/qq_27825451/article/details/102457058)
3. Wiki： [Perplexity](https://en.wikipedia.org/wiki/Perplexity)

